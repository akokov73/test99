{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjV8eDbShJBZKxXrQNrbDN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akokov73/test99/blob/main/GetDataFromSite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGq6pZbngL1T",
        "outputId": "512f163c-00c8-435c-e8c1-99d87f8839a9"
      },
      "source": [
        "! pip3 install requests # импортируем модуль requests"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2t1omBNgQyS"
      },
      "source": [
        "! pip3 install beautifulsoup4 \r\n",
        "! pip install cfscrape\r\n",
        "! pip3 list\r\n",
        "import os\r\n",
        "import time\r\n",
        "import random\r\n",
        "import requests as rq\r\n",
        "import cfscrape\r\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzJegFaVAANw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv36b7wugQ_o"
      },
      "source": [
        "url = 'https://med-books.by/'\r\n",
        "# url = 'https://med-books.by/istorii_nefrologiya/'\r\n",
        "# url = 'https://med-books.by/istorii_vnutrennie-bolezni/5833-istoriya-bolezni-hronicheskiy-pielonefrit-latentnoe-techenie-simptomaticheskaya-gipertoniya.html'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl7uyRk2i2ir"
      },
      "source": [
        "\r\n",
        "Titles_articles = []\r\n",
        "scraper = cfscrape.create_scraper()  # returns a CloudflareScraper instance\r\n",
        "page = scraper.get(url).content  # => \"<!DOCTYPE html><html><head>...\"\r\n",
        "soup = BeautifulSoup(page,\"html.parser\")\r\n",
        "for a_tag in soup.find_all(\"a\"):\r\n",
        "    if 'istorii' in a_tag['href']:\r\n",
        "        if not 'https' in a_tag['href']:\r\n",
        "            print(a_tag['href'])\r\n",
        "            Titles_articles.append(a_tag['href']) \r\n",
        "print(Titles_articles)            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ61SHe51xK0",
        "outputId": "c35eb688-8b01-4182-ccb5-2fc0a669a05c"
      },
      "source": [
        "art_url = Titles_articles[0][1:-1]\r\n",
        "print(art_url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "istorii_akusherstvo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rOfoLj_PtFhU"
      },
      "source": [
        "for art_url in Titles_articles: \r\n",
        "    new_art_url = url[:-1] + art_url\r\n",
        "    time.sleep(random.randint(3,4))\r\n",
        "    scraper = cfscrape.create_scraper()  # returns a CloudflareScraper instance\r\n",
        "    page = scraper.get(new_art_url).content  # => \"<!DOCTYPE html><html><head>...\"\r\n",
        "    soup = BeautifulSoup(page,\"html.parser\")\r\n",
        "    Titles = []\r\n",
        "    name_of_file = art_url[1:-1] + '.txt'\r\n",
        "    my_file = open(name_of_file, 'w')\r\n",
        "    for tag in soup.find_all(\"div\",{\"class\":\"all\"}): \r\n",
        "        for tag_1 in tag.find_all(\"h2\",{\"class\":\"title\"}): \r\n",
        "            for tag_2 in tag.find_all(\"a\"): \r\n",
        "                url_child = tag_2['href']\r\n",
        "                if 'html' in url_child:\r\n",
        "                    # print (url_child )\r\n",
        "                    # print(tag_2['href'])\r\n",
        "                    number_str = len(Titles)+1\r\n",
        "                    my_file.write(str(number_str) +'\\n')                \r\n",
        "                    my_file.write(tag_2['href'])\r\n",
        "                    my_file.write('\\n')                \r\n",
        "                    Titles.append(tag_2['href'])\r\n",
        "                    time.sleep(random.randint(3,4))\r\n",
        "                    page_child = scraper.get(url_child).content  # => \"<!DOCTYPE html><html><head>...\"\r\n",
        "                    soup_child = BeautifulSoup(page_child,\"html.parser\")    \r\n",
        "                    for tag_3 in soup_child.find_all(\"div\",{\"class\":\"news\"}):                 \r\n",
        "                        my_file.write(tag_3.text)\r\n",
        "                        print(tag_3.text)\r\n",
        "                    my_file.write('\\n')\r\n",
        "                    \r\n",
        "    # print(Titles)\r\n",
        "    my_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_banJKgrrfSf"
      },
      "source": [
        "! cat test_data.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ud3ykmKgyHJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXOkMNFygyPX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}